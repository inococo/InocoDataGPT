{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Federated Data LLMs with the centralized incremental vector store to fine-tune distributed GPTs**\n\nFederation relationship:\n1. Centralized vector store (sharing data) and GPT-4 (knowledge)\n2. Edges: fine-tuned GPT-J and enterprise data warehouses\n\nFederation logic and algorithms:\n1. The vector store contains both private data (like data catalog) and shareable data (like filtered operational data)\n2. The LLM vectors from prompts to GPT-4\n3. Fine-tune GPT-J with vector data\n\nFine-tune GPT-J specifically in three areas:\n1. Froze model with low-rank adapters (LoRA) with 8-bit backbone\n2. Train from knowledge checkpoint (for globally sharing)\n3. Train from vector data checkpoint (for incremental edge)\n\n\nThis notebook is a proof of concept for fine-tuning GPT-J only due to external connectors and resource limitations\n\nGPT-J fine-tuning References:\n* In March 2023, Databricks released Dolly, an Apache-licensed, instruction-following model based on GPT-J with fine-tuning from the Stanford Alpaca dataset: https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\n* Fine-tuning 6B GPT-J in Colab: https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es\n* Fine-tuning GPT-J-6B in Github thread: https://github.com/huggingface/transformers/issues/14839\n* GPT-J-6B Fine-Tuning with Ray AIR: https://docs.ray.io/en/latest/ray-air/examples/gptj_deepspeed_fine_tuning.html","metadata":{}},{"cell_type":"code","source":"# Comment out for installing once\n\n#! pip install \"datasets\" \"evaluate\" \"accelerate>=0.16.0\" \"transformers>=4.26.0\" \"torch>=1.12.0\" \"deepspeed\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.cuda.amp import custom_fwd, custom_bwd\n\nfrom bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n\nfrom tqdm.auto import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n \n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n \n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n \n \nclass DequantizeAndLinear(torch.autograd.Function): \n    @staticmethod\n    @custom_fwd\n    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n \n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_qu